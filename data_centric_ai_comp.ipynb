{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_centric_ai_comp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOSenyJlArPDXNpCeg2TJMi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tcole333/data-centric-ai/blob/tcole/data_centric_ai_comp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgLzoq8ndlTm"
      },
      "source": [
        "import requests\n",
        "import tarfile\n",
        "import io\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from tensorflow.python.keras.preprocessing import dataset_utils\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import json\n",
        "import sys"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Dp2H0C3sk14",
        "outputId": "49ce0cb5-ca32-491c-8bb7-ab5a61933920",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#clear directories so you have a fresh start\n",
        "!rm -r '/content/data'\n",
        "!rm -r '/content/data_preprocessed'"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/data': No such file or directory\n",
            "rm: cannot remove '/content/data_preprocessed': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8dAnIhSdZ14",
        "outputId": "fc93983a-e724-482a-fff5-fd251a760ec0"
      },
      "source": [
        "#download the files from github and extract them and move the sample labels to a new folder called test\n",
        "!wget https://github.com/tcole333/data-centric-ai/blob/main/data.tar.gz?raw=true\n",
        "tar = tarfile.open('data.tar.gz?raw=true')\n",
        "tar.extractall()\n",
        "!wget https://github.com/tcole333/data-centric-ai/blob/main/label_book.tar.gz?raw=true\n",
        "tar = tarfile.open('label_book.tar.gz?raw=true')\n",
        "tar.extractall()\n",
        "#set up the folder structure for the preprocessed data\n",
        "!mv \"/content/label_book\" \"/content/data/test\"\n",
        "!mkdir '/content/data_preprocessed'\n",
        "!mkdir '/content/data_preprocessed/train'\n",
        "!mkdir '/content/data_preprocessed/val'\n",
        "!for num in i ii iii iv v vi vii viii ix x; do mkdir /content/data_preprocessed/train/$num ; mkdir /content/data_preprocessed/val/$num; done\n",
        "!cp -r '/content/data/test' '/content/data_preprocessed/test'\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-23 19:56:07--  https://github.com/tcole333/data-centric-ai/blob/main/data.tar.gz?raw=true\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/tcole333/data-centric-ai/raw/main/data.tar.gz [following]\n",
            "--2021-06-23 19:56:07--  https://github.com/tcole333/data-centric-ai/raw/main/data.tar.gz\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/tcole333/data-centric-ai/main/data.tar.gz [following]\n",
            "--2021-06-23 19:56:07--  https://raw.githubusercontent.com/tcole333/data-centric-ai/main/data.tar.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4115570 (3.9M) [application/octet-stream]\n",
            "Saving to: ‘data.tar.gz?raw=true.4’\n",
            "\n",
            "data.tar.gz?raw=tru 100%[===================>]   3.92M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-06-23 19:56:08 (31.5 MB/s) - ‘data.tar.gz?raw=true.4’ saved [4115570/4115570]\n",
            "\n",
            "--2021-06-23 19:56:08--  https://github.com/tcole333/data-centric-ai/blob/main/label_book.tar.gz?raw=true\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/tcole333/data-centric-ai/raw/main/label_book.tar.gz [following]\n",
            "--2021-06-23 19:56:09--  https://github.com/tcole333/data-centric-ai/raw/main/label_book.tar.gz\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/tcole333/data-centric-ai/main/label_book.tar.gz [following]\n",
            "--2021-06-23 19:56:09--  https://raw.githubusercontent.com/tcole333/data-centric-ai/main/label_book.tar.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 82328 (80K) [application/octet-stream]\n",
            "Saving to: ‘label_book.tar.gz?raw=true.4’\n",
            "\n",
            "label_book.tar.gz?r 100%[===================>]  80.40K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2021-06-23 19:56:09 (15.3 MB/s) - ‘label_book.tar.gz?raw=true.4’ saved [82328/82328]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwHd1UkctAKZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "tG3kVe_pg44A",
        "outputId": "1956309f-1c8f-4e09-eabf-3e220d359c11"
      },
      "source": [
        "#read a file and convert to an array and back to an image\n",
        "img = load_img('/content/data_preprocessed/test/ii/a37502ee-ce5d-11eb-b317-38f9d35ea60f.png')\n",
        "img_numpy_array = img_to_array(img)\n",
        "print('shape', img_numpy_array.shape)\n",
        "#print(img_numpy_array)\n",
        "img_pil_from_numpy_array = array_to_img(img_numpy_array)\n",
        "img_pil_from_numpy_array"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape (186, 195, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMMAAAC6CAIAAACcMG70AAAFqklEQVR4nO3d3ZKkKBRFYXKiHwyfXN/MuSDCtv1LlI2ck6zvqmempsIoVwOiaX3meQ5Asf9aHwB+BCVBg5KgQUnQoCRoUBI0KKmiYRhaH8J7/rQ+gN80DMM0Ta2P4lWMSWLDMHw+nyWjfoYlxiSlz+fT+hCaYUyS6Wf4OURJGocLoxjjOI4tDqcBShLocH29R0mlzjLqakAKIXx4quSx66Gotx8sY9JDZLTBLsA9ywUaC6MNSrqBlfUFZrcbMjOKMdY9DpMoKUu6B9L6KExjdvuCGS0TuwCnMhta5rLli/v8kTK7HcvJKMa4ZDSOY/pznxmFEMKMf2Wul2OMy9ev/99GR90es9tfaa8oZyjq6jZIJhMr7mEYWp2bWzuNabgio0ONS1pOZLrGfvOv+92LMoaiay1nt/25rP2XPnP+2iOjr5qVdPZoWPqD9rQ9C2i99Cajr0ysk/YKV07rB2EfB0Q9txgt6fFZLNySZhZ7zOjO5N2n69N9sfXHg+5KW0Fk9JitMWnpIMaYH1PhfTGu7SVaXru1vbvOekir8R73elkTY6x915166jE0u9XL6HD+WmZPqpIwVJJWGuG+roHISOWnStrcxj+buNNoRENa7Z8FKFl3sw1tR/sxaV3DxVJp/9gQ6ZjSfkxaWy7lDh83G8eRZbJZtkpKlhuu3LtwxGJJScPH3/CA3ZLgi9E7uHCHkqBBSdDovaTlwabO3zdaruuS+My/UNclrU3TxLBUgpKgQUl/MdOVoCRodL3HvX+gpeefRiHGJGhQ0j+4fHus35KIRqvfkqDV/unbHJvxo95zS2wEPObj2m15T1f6R0lJZ59EcPEDMcjB7LZ/gR9LHIMclLQmnH36/J0Q9TgoaVMPSxmbHJQEF6yX9PKSiCnvMesl7XGybbJe0uGqSLILwHpLy3RJXO07YrqkemhUrtOSzvD58cf8lcTJtslfSbCp05IOL9zYXyjRY0kst2swXRJbPo6YLmmPCcgsZyVVxVVhCUqCBiVBw+5z3IevpJEcLU9w19DdmHS2BcBavpCzktgKMstZSZUwIJXrrqSz3U62AArZLane05KowW5Je5I56PCbcFumnKeSYBklQYOSoEFJIbALoEBJ0KAkaFBSCGxTKVASiySR2arDo40xln/ndTqSb4h5nu2OSfWGinEc53lODTGvqdgtqfY5piEtu89Mhsvf5Kd9Dy7K+Xgf9x73XK2xO7uFEMZxPFstLf+epyiNMD27LdbP8DOv2eSjpLAaewjIJjclwTjT6yQ4QknQoCRoUBI0fOxMXmwafd2iXO9ILdd96RtyGShk69ptU8wLG9mHneGBliVd3FZrZb+lTl6ZGpS0/x2SxsUY6emrV0ty19DGZsQir7X3SvKe0RlGrOSlkuotiZZxom2g9PRGSWdv48t09mBJ/pk73ESoUV461D6TslVSq0sn+czb4RD1Rkmbqe3iUX8LP31VVb3F1GDF7evnWxhWPz3Z2uM27llVnSyeKOmhVFV+Uj8/OFFSkbuj1A/3REkat4aon+yJkpR67omS9PrsiZJqye/pN2KipOoy7zl674nnuKu7+Ez62jRNrj+ZTklvWN7YdP1lrmOipPfkDE5+Y6KkV+UMTk5jYsXdxtdluLvzwpjUxteZzt2wxJjU0vXI5OvUMCa1lJZNZ//V17BESe39xqvlmd1MOJvmHJ0dxiQTzu6TOJrgKMkK73McJUGDkqzIvNFrFiWZ5ug5E0qyy9GFW/DydsBOpBHI0fXaGvtJ0GB2gwYlQYOSoEFJ0KAkaFASNCgJGpQEDUqCBiVBg5KgQUnQoCRoUBI0KAkalAQNSoIGJUGDkqBBSdCgJGhQEjQoCRqUBA1KggYlQYOSoNGyJKevUsAh3jABDWY3aFASNCgJGpQEDUqCBiVBg5KgQUnQoCRoUBI0KAkalASN/wExMOKdoWgFfQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=195x186 at 0x7F46B1565ED0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SscP4-tPdVqx",
        "outputId": "b32de5ba-9c1c-4c9b-a489-13c2292214e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "img = load_img('abb66614-ce5d-11eb-b317-38f9d35ea60f.png')\n",
        "display(img)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-eb05bb64da9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'abb66614-ce5d-11eb-b317-38f9d35ea60f.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    291\u001b[0m   \"\"\"\n\u001b[1;32m    292\u001b[0m   return image.load_img(path, grayscale=grayscale, color_mode=color_mode,\n\u001b[0;32m--> 293\u001b[0;31m                         target_size=target_size, interpolation=interpolation)\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    111\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    112\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'abb66614-ce5d-11eb-b317-38f9d35ea60f.png'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdTAiFV6lVa3"
      },
      "source": [
        "train_datagen = ImageDataGenerator(rotation_range=20,\n",
        "                                   width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   rescale=1/255,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=False,\n",
        "                                   fill_mode='nearest')\n",
        "\n",
        "valid_datagen = ImageDataGenerator(rescale=1/255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5X4sxy-OdLKR"
      },
      "source": [
        "def set_label(path):\n",
        "  #list of tuples where the first item is the path to the image and the second is the correct class for it\n",
        "  mislabeled_images = []\n",
        "  #loop through all the images and get user input on them\n",
        "  for f in os.listdir(path):\n",
        "    img = load_img(path + '/' + f)\n",
        "    display(img)\n",
        "    user_class = input(\"Is this image in the correct class? \\n Enter y if yes, otherwise enter the correct class (i,ii,vii,etc.): \")\n",
        "    if user_class == 'end':\n",
        "      break \n",
        "    elif user_class != 'y':\n",
        "      mislabeled_images.append((f, user_class))\n",
        "  return mislabeled_images\n",
        "     \n"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cN30lblMeXOZ",
        "outputId": "d8f81938-55bb-4965-c7f4-e7cc190d6692",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        }
      },
      "source": [
        "set_label('/content/data/train/i')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAN4AAAD8CAIAAACXYY3dAAAEf0lEQVR4nO3cTY7aShhA0fRTNsbKvTQyeFIGQMAY/9zC5wxbUdqxL1/ZReDner3+gp7/jj4AeEyaREmTKGkSJU2ipEmUNImSJlHSJEqaREmTKGkSJU2ipEmUNImSJlHSJEqaREmTKGkSJU2ipEmUNImSJlHSJEqaREmTKGkSJU2ipEmUNImSJlHSJEqaREmTKGkSJU2ipEmUNImSJlHSJEqaREmTKGkSJU2ipEmUNImSJlHSJEqaREmTKGkSJU2ipEmUNImSJlHSJEqaREmTKGkSJU2ipEmUNImSJlHSJEqaREmTKGkSJU2ipEmUNImSJlHSJEqaRP0++gBeu1wuc/7YNE1bHwl7GiDNme4Lnqbp5ofyHcjP9Xo9+hhemDk15xPoEL5nas73pHXVdpwxzSce3hUcciRI8wV3q0eR5nuM1d1I81PPn9KEu5gt922tvr1wHgNsHv019GW2yfqukdJ8bvRwjz6EHPeaCQ9fVyfv9XvSfPdCDj1lz+B7FvR1lcM9yTT1hP7YSS5/2fcs6Ku7qbM8R7+SNOd6PkeFuzppruNhuHr9hDQ39GTQqvYlaR5DtS/ZPCLK5hFR0iRKmkRJkyhpEiVNoqRJlDSJkiZR3qh8wOfLCkzNW97CjjA1ZzFH92dqvnY/R03WHUhzIXVuTZrLqXNT0vzI5XIR6EY8Bt36+4ijuWOZmiswO7cgTaJ8NuiFt8ah/c4VSXOu+Y0KdBUW9LkEtzNpEiVNoqT5Bmv6nqRJlDTXZ/t9FdJ8jzV9N9IkSppESfNtc9Z0t5ufkyZR0lzC4NyBNBfyqL41aRIlzQ1Z0z8hzeWs6ZuSJlHS3JY1fTFpEiXNj7jd3I40iZLmpwzOjUhzc56ElpEmUdLcg8G5gDSJkiZR0lyB/765BWkSJU2ipLkOG++rkyZR0iRKmkRJczXTNLnjXJE0V/akTlubb5EmUdJcmdG4FmkSJU2ipEmUNImS5n7ser5FmkRJkyhprsyqvRZpEiVNoqRJlDSJkiZR0iRKmkRJcyf2O98lTaKkSZQ0iZImUdIkSpore/iJSo/nC0iTKGluzshcRppESZOon+v1evQxwAOmJlHSJEqaREmTKGkSJU2ipEmUNImSJlHSJEqaREmTKGkSJU2ipEmUNImSJlHSJEqaREmTKGkSJU2ipEmUNImSJlHSJEqaREmTKGkS9fvoA1jB/XdU+7bVLzB8mg+/O50bc87S/6/nBedzo0Ew6vdrPj+DZ56ah7xWtzjhQ07NE07KE/6Th0xzIDdJLV40T2iYNI+9nGv9dlHON0aa86/oy5secYxijDRf+leRQhzXAGl6GA/a4bQPkOZz9tu/1fBp3rOILzNN08P9hKN8YZq89K/mUguONL9HKqzPSXMwX9bfE2O8h36S28fzZDfHGGn+WrrrflTTIvvcMAv6w4u9eOfo/ml02d/DdoaZmpyND2AQJU2ipEmUNImSJlHSJEqaREmTKGkSJU2ipEmUNImSJlHSJEqaREmTKGkSJU2ipEmUNImSJlHSJEqaRP0BYkv5ieJ39nIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=222x252 at 0x7F46B2D3D650>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_JggeQ0QJsc"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTzOkz7meEZ5",
        "outputId": "51086387-a9b4-46e0-8a8a-2c356de4911e"
      },
      "source": [
        "class_names=[\"i\", \"ii\", \"iii\", \"iv\", \"v\", \"vi\", \"vii\", \"viii\", \"ix\", \"x\"]\n",
        "\n",
        "for class_name in class_names:\n",
        "  train_generator = train_datagen.flow_from_directory(\n",
        "    '/content/data/train',\n",
        "    target_size=(32,32),\n",
        "    batch_size=500,\n",
        "    classes=[class_name],\n",
        "    save_to_dir='/content/data_preprocessed/train/'+class_name,\n",
        "    save_prefix='aug'\n",
        "  )\n",
        "  batch = next(train_generator)\n",
        "  \n",
        "for class_name in class_names:\n",
        "  batch_size = len([f for f in os.listdir('/content/data/val/'+ class_name) if os.path.isfile(os.path.join('/conent/data/val/' + class_name, f))])\n",
        "  validation_generator = valid_datagen.flow_from_directory(\n",
        "        '/content/data/val',\n",
        "        target_size=(32, 32),\n",
        "        class_mode='categorical',\n",
        "        classes=[class_name],\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        save_to_dir='/content/data_preprocessed/val/'+class_name)  \n",
        "  batch = next(validation_generator)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i\n",
            "Found 261 images belonging to 1 classes.\n",
            "ii\n",
            "Found 157 images belonging to 1 classes.\n",
            "iii\n",
            "Found 186 images belonging to 1 classes.\n",
            "iv\n",
            "Found 281 images belonging to 1 classes.\n",
            "v\n",
            "Found 196 images belonging to 1 classes.\n",
            "vi\n",
            "Found 181 images belonging to 1 classes.\n",
            "vii\n",
            "Found 193 images belonging to 1 classes.\n",
            "viii\n",
            "Found 199 images belonging to 1 classes.\n",
            "ix\n",
            "Found 234 images belonging to 1 classes.\n",
            "x\n",
            "Found 179 images belonging to 1 classes.\n",
            "Found 82 images belonging to 1 classes.\n",
            "Found 81 images belonging to 1 classes.\n",
            "Found 79 images belonging to 1 classes.\n",
            "Found 84 images belonging to 1 classes.\n",
            "Found 83 images belonging to 1 classes.\n",
            "Found 82 images belonging to 1 classes.\n",
            "Found 77 images belonging to 1 classes.\n",
            "Found 83 images belonging to 1 classes.\n",
            "Found 81 images belonging to 1 classes.\n",
            "Found 81 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Pbq82-FiOlv",
        "outputId": "2a06f1b1-baaf-48aa-b30a-72b6f99842b6"
      },
      "source": [
        "directory = \"/content/data_preprocessed\"\n",
        "user_data = directory + \"/train\"\n",
        "valid_data = directory + \"/val\"\n",
        "test_data = directory + \"/test\" # this can be the label book, or any other test set you create\n",
        "\n",
        "### DO NOT MODIFY BELOW THIS LINE, THIS IS THE FIXED MODEL ###\n",
        "batch_size = 8\n",
        "tf.random.set_seed(123)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        user_data,# + '/train',\n",
        "        labels=\"inferred\",\n",
        "        label_mode=\"categorical\",\n",
        "        class_names=[\"i\", \"ii\", \"iii\", \"iv\", \"v\", \"vi\", \"vii\", \"viii\", \"ix\", \"x\"],\n",
        "        shuffle=True,\n",
        "        seed=123,\n",
        "        batch_size=batch_size,\n",
        "        image_size=(32, 32),\n",
        "    )\n",
        "\n",
        "    valid = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        user_data,# + '/val',\n",
        "        labels=\"inferred\",\n",
        "        label_mode=\"categorical\",\n",
        "        class_names=[\"i\", \"ii\", \"iii\", \"iv\", \"v\", \"vi\", \"vii\", \"viii\", \"ix\", \"x\"],\n",
        "        shuffle=True,\n",
        "        seed=123,\n",
        "        batch_size=batch_size,\n",
        "        image_size=(32, 32),\n",
        "    )\n",
        "\n",
        "    total_length = ((train.cardinality() + valid.cardinality()) * batch_size).numpy()\n",
        "    if total_length > 10_000:\n",
        "        print(f\"Dataset size larger than 10,000. Got {total_length} examples\")\n",
        "        sys.exit()\n",
        "\n",
        "    test = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        test_data,\n",
        "        labels=\"inferred\",\n",
        "        label_mode=\"categorical\",\n",
        "        class_names=[\"i\", \"ii\", \"iii\", \"iv\", \"v\", \"vi\", \"vii\", \"viii\", \"ix\", \"x\"],\n",
        "        shuffle=False,\n",
        "        seed=123,\n",
        "        batch_size=batch_size,\n",
        "        image_size=(32, 32),\n",
        "    )\n",
        "\n",
        "    base_model = tf.keras.applications.ResNet50(\n",
        "        input_shape=(32, 32, 3),\n",
        "        include_top=False,\n",
        "        weights=None,\n",
        "    )\n",
        "    base_model = tf.keras.Model(\n",
        "        base_model.inputs, outputs=[base_model.get_layer(\"conv2_block3_out\").output]\n",
        "    )\n",
        "\n",
        "    inputs = tf.keras.Input(shape=(32, 32, 3))\n",
        "    x = tf.keras.applications.resnet.preprocess_input(inputs)\n",
        "    x = base_model(x)\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "    x = tf.keras.layers.Dense(10)(x)\n",
        "    model = tf.keras.Model(inputs, x)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n",
        "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    model.summary()\n",
        "    loss_0, acc_0 = model.evaluate(valid)\n",
        "    print(f\"loss {loss_0}, acc {acc_0}\")\n",
        "\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "        \"best_model\",\n",
        "        monitor=\"val_accuracy\",\n",
        "        mode=\"max\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        train,\n",
        "        validation_data=valid,\n",
        "        epochs=100,\n",
        "        callbacks=[checkpoint],\n",
        "    )\n",
        "\n",
        "    model.load_weights(\"best_model\")\n",
        "\n",
        "    loss, acc = model.evaluate(valid)\n",
        "    print(f\"final loss {loss}, final acc {acc}\")\n",
        "\n",
        "    test_loss, test_acc = model.evaluate(test)\n",
        "    print(f\"test loss {test_loss}, test acc {test_acc}\")\n",
        "\n",
        "   "
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1000 files belonging to 10 classes.\n",
            "Found 1000 files belonging to 10 classes.\n",
            "Found 52 files belonging to 10 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "tf.__operators__.getitem (Sl (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add (TFOpLambda)  (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "model (Functional)           (None, 8, 8, 256)         229760    \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 232,330\n",
            "Trainable params: 229,386\n",
            "Non-trainable params: 2,944\n",
            "_________________________________________________________________\n",
            "125/125 [==============================] - 3s 14ms/step - loss: 32.9953 - accuracy: 0.1000\n",
            "loss 32.995330810546875, acc 0.10000000149011612\n",
            "Epoch 1/100\n",
            "125/125 [==============================] - 9s 58ms/step - loss: 2.3159 - accuracy: 0.1470 - val_loss: 2.4265 - val_accuracy: 0.1000\n",
            "Epoch 2/100\n",
            "125/125 [==============================] - 7s 57ms/step - loss: 2.0120 - accuracy: 0.3010 - val_loss: 2.0217 - val_accuracy: 0.2380\n",
            "Epoch 3/100\n",
            "125/125 [==============================] - 7s 57ms/step - loss: 1.8261 - accuracy: 0.3830 - val_loss: 1.6786 - val_accuracy: 0.4300\n",
            "Epoch 4/100\n",
            "125/125 [==============================] - 7s 57ms/step - loss: 1.6662 - accuracy: 0.4410 - val_loss: 1.4453 - val_accuracy: 0.5410\n",
            "Epoch 5/100\n",
            "125/125 [==============================] - 7s 57ms/step - loss: 1.5496 - accuracy: 0.5000 - val_loss: 1.3373 - val_accuracy: 0.5880\n",
            "Epoch 6/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 1.3853 - accuracy: 0.5700 - val_loss: 1.2505 - val_accuracy: 0.5860\n",
            "Epoch 7/100\n",
            "125/125 [==============================] - 7s 59ms/step - loss: 1.2570 - accuracy: 0.6270 - val_loss: 1.3280 - val_accuracy: 0.5250\n",
            "Epoch 8/100\n",
            "125/125 [==============================] - 7s 59ms/step - loss: 1.1362 - accuracy: 0.6670 - val_loss: 0.9056 - val_accuracy: 0.7460\n",
            "Epoch 9/100\n",
            "125/125 [==============================] - 7s 60ms/step - loss: 1.0076 - accuracy: 0.7260 - val_loss: 0.8256 - val_accuracy: 0.7420\n",
            "Epoch 10/100\n",
            "125/125 [==============================] - 8s 61ms/step - loss: 0.8891 - accuracy: 0.7660 - val_loss: 0.7587 - val_accuracy: 0.7560\n",
            "Epoch 11/100\n",
            "125/125 [==============================] - 8s 60ms/step - loss: 0.7849 - accuracy: 0.8030 - val_loss: 0.6861 - val_accuracy: 0.7830\n",
            "Epoch 12/100\n",
            "125/125 [==============================] - 7s 59ms/step - loss: 0.7139 - accuracy: 0.8130 - val_loss: 0.9557 - val_accuracy: 0.6460\n",
            "Epoch 13/100\n",
            "125/125 [==============================] - 7s 59ms/step - loss: 0.5730 - accuracy: 0.8740 - val_loss: 0.4586 - val_accuracy: 0.8940\n",
            "Epoch 14/100\n",
            "125/125 [==============================] - 7s 59ms/step - loss: 0.4630 - accuracy: 0.9250 - val_loss: 0.3403 - val_accuracy: 0.9380\n",
            "Epoch 15/100\n",
            "125/125 [==============================] - 7s 59ms/step - loss: 0.4183 - accuracy: 0.9240 - val_loss: 0.3818 - val_accuracy: 0.9020\n",
            "Epoch 16/100\n",
            "125/125 [==============================] - 7s 59ms/step - loss: 0.3879 - accuracy: 0.9400 - val_loss: 0.3492 - val_accuracy: 0.9090\n",
            "Epoch 17/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.3082 - accuracy: 0.9550 - val_loss: 0.3412 - val_accuracy: 0.9000\n",
            "Epoch 18/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.2615 - accuracy: 0.9640 - val_loss: 0.2615 - val_accuracy: 0.9410\n",
            "Epoch 19/100\n",
            "125/125 [==============================] - 7s 59ms/step - loss: 0.2429 - accuracy: 0.9630 - val_loss: 0.1471 - val_accuracy: 0.9850\n",
            "Epoch 20/100\n",
            "125/125 [==============================] - 7s 59ms/step - loss: 0.1997 - accuracy: 0.9730 - val_loss: 0.1658 - val_accuracy: 0.9660\n",
            "Epoch 21/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.1876 - accuracy: 0.9730 - val_loss: 0.0870 - val_accuracy: 0.9970\n",
            "Epoch 22/100\n",
            "125/125 [==============================] - 7s 59ms/step - loss: 0.1462 - accuracy: 0.9790 - val_loss: 0.2194 - val_accuracy: 0.9430\n",
            "Epoch 23/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.1577 - accuracy: 0.9810 - val_loss: 0.1142 - val_accuracy: 0.9810\n",
            "Epoch 24/100\n",
            "125/125 [==============================] - 7s 57ms/step - loss: 0.1464 - accuracy: 0.9850 - val_loss: 0.3717 - val_accuracy: 0.8590\n",
            "Epoch 25/100\n",
            "125/125 [==============================] - 7s 57ms/step - loss: 0.1501 - accuracy: 0.9770 - val_loss: 0.1168 - val_accuracy: 0.9800\n",
            "Epoch 26/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.1265 - accuracy: 0.9820 - val_loss: 0.1223 - val_accuracy: 0.9690\n",
            "Epoch 27/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.1215 - accuracy: 0.9830 - val_loss: 0.4588 - val_accuracy: 0.8350\n",
            "Epoch 28/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0970 - accuracy: 0.9950 - val_loss: 0.0292 - val_accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0865 - accuracy: 0.9910 - val_loss: 0.0306 - val_accuracy: 0.9980\n",
            "Epoch 30/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0858 - accuracy: 0.9930 - val_loss: 0.1563 - val_accuracy: 0.9600\n",
            "Epoch 31/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.1027 - accuracy: 0.9860 - val_loss: 0.0649 - val_accuracy: 0.9890\n",
            "Epoch 32/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.1070 - accuracy: 0.9870 - val_loss: 0.2736 - val_accuracy: 0.8990\n",
            "Epoch 33/100\n",
            "125/125 [==============================] - 7s 59ms/step - loss: 0.0923 - accuracy: 0.9850 - val_loss: 0.0681 - val_accuracy: 0.9870\n",
            "Epoch 34/100\n",
            "125/125 [==============================] - 7s 59ms/step - loss: 0.0994 - accuracy: 0.9830 - val_loss: 0.1296 - val_accuracy: 0.9710\n",
            "Epoch 35/100\n",
            "125/125 [==============================] - 7s 59ms/step - loss: 0.0866 - accuracy: 0.9870 - val_loss: 0.0438 - val_accuracy: 0.9970\n",
            "Epoch 36/100\n",
            "125/125 [==============================] - 7s 59ms/step - loss: 0.0677 - accuracy: 0.9940 - val_loss: 0.0400 - val_accuracy: 0.9950\n",
            "Epoch 37/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0785 - accuracy: 0.9850 - val_loss: 0.1227 - val_accuracy: 0.9670\n",
            "Epoch 38/100\n",
            "125/125 [==============================] - 7s 57ms/step - loss: 0.0914 - accuracy: 0.9850 - val_loss: 0.1002 - val_accuracy: 0.9740\n",
            "Epoch 39/100\n",
            "125/125 [==============================] - 7s 57ms/step - loss: 0.0788 - accuracy: 0.9890 - val_loss: 0.0354 - val_accuracy: 0.9970\n",
            "Epoch 40/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0792 - accuracy: 0.9860 - val_loss: 0.3720 - val_accuracy: 0.8510\n",
            "Epoch 41/100\n",
            "125/125 [==============================] - 7s 57ms/step - loss: 0.0716 - accuracy: 0.9860 - val_loss: 0.0382 - val_accuracy: 0.9940\n",
            "Epoch 42/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0559 - accuracy: 0.9920 - val_loss: 0.0816 - val_accuracy: 0.9810\n",
            "Epoch 43/100\n",
            "125/125 [==============================] - 7s 57ms/step - loss: 0.0460 - accuracy: 0.9960 - val_loss: 0.0365 - val_accuracy: 0.9910\n",
            "Epoch 44/100\n",
            "125/125 [==============================] - 7s 57ms/step - loss: 0.0453 - accuracy: 0.9960 - val_loss: 0.0919 - val_accuracy: 0.9660\n",
            "Epoch 45/100\n",
            "125/125 [==============================] - 7s 57ms/step - loss: 0.0673 - accuracy: 0.9890 - val_loss: 0.0408 - val_accuracy: 0.9930\n",
            "Epoch 46/100\n",
            "125/125 [==============================] - 7s 59ms/step - loss: 0.0929 - accuracy: 0.9810 - val_loss: 0.3599 - val_accuracy: 0.8790\n",
            "Epoch 47/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0573 - accuracy: 0.9920 - val_loss: 0.0481 - val_accuracy: 0.9880\n",
            "Epoch 48/100\n",
            "125/125 [==============================] - 7s 57ms/step - loss: 0.0846 - accuracy: 0.9840 - val_loss: 0.6523 - val_accuracy: 0.7800\n",
            "Epoch 49/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0562 - accuracy: 0.9910 - val_loss: 0.0885 - val_accuracy: 0.9750\n",
            "Epoch 50/100\n",
            "125/125 [==============================] - 7s 57ms/step - loss: 0.0442 - accuracy: 0.9950 - val_loss: 0.0441 - val_accuracy: 0.9940\n",
            "Epoch 51/100\n",
            "125/125 [==============================] - 7s 57ms/step - loss: 0.0415 - accuracy: 0.9950 - val_loss: 0.1562 - val_accuracy: 0.9410\n",
            "Epoch 52/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0497 - accuracy: 0.9910 - val_loss: 0.0637 - val_accuracy: 0.9840\n",
            "Epoch 53/100\n",
            "125/125 [==============================] - 7s 59ms/step - loss: 0.0632 - accuracy: 0.9870 - val_loss: 0.2770 - val_accuracy: 0.9120\n",
            "Epoch 54/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0461 - accuracy: 0.9920 - val_loss: 0.0120 - val_accuracy: 0.9990\n",
            "Epoch 55/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0501 - accuracy: 0.9900 - val_loss: 0.0298 - val_accuracy: 0.9950\n",
            "Epoch 56/100\n",
            "125/125 [==============================] - 7s 57ms/step - loss: 0.0425 - accuracy: 0.9940 - val_loss: 0.0072 - val_accuracy: 0.9990\n",
            "Epoch 57/100\n",
            "125/125 [==============================] - 7s 57ms/step - loss: 0.0459 - accuracy: 0.9910 - val_loss: 0.9682 - val_accuracy: 0.7380\n",
            "Epoch 58/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0372 - accuracy: 0.9930 - val_loss: 0.0233 - val_accuracy: 0.9970\n",
            "Epoch 59/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0579 - accuracy: 0.9890 - val_loss: 0.0525 - val_accuracy: 0.9850\n",
            "Epoch 60/100\n",
            "125/125 [==============================] - 7s 59ms/step - loss: 0.0651 - accuracy: 0.9810 - val_loss: 0.2968 - val_accuracy: 0.8980\n",
            "Epoch 61/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0442 - accuracy: 0.9920 - val_loss: 0.0310 - val_accuracy: 0.9920\n",
            "Epoch 62/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0374 - accuracy: 0.9940 - val_loss: 0.0107 - val_accuracy: 0.9990\n",
            "Epoch 63/100\n",
            "125/125 [==============================] - 7s 59ms/step - loss: 0.0312 - accuracy: 0.9970 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0221 - accuracy: 0.9990 - val_loss: 0.0246 - val_accuracy: 0.9950\n",
            "Epoch 65/100\n",
            "125/125 [==============================] - 7s 57ms/step - loss: 0.0301 - accuracy: 0.9950 - val_loss: 0.0281 - val_accuracy: 0.9980\n",
            "Epoch 66/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0640 - accuracy: 0.9830 - val_loss: 0.0294 - val_accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0665 - accuracy: 0.9840 - val_loss: 0.3874 - val_accuracy: 0.8510\n",
            "Epoch 68/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0328 - accuracy: 0.9950 - val_loss: 0.0053 - val_accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "125/125 [==============================] - 7s 60ms/step - loss: 0.0409 - accuracy: 0.9950 - val_loss: 0.0574 - val_accuracy: 0.9820\n",
            "Epoch 70/100\n",
            "125/125 [==============================] - 7s 59ms/step - loss: 0.0234 - accuracy: 0.9990 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "125/125 [==============================] - 7s 59ms/step - loss: 0.0259 - accuracy: 0.9960 - val_loss: 0.0265 - val_accuracy: 0.9940\n",
            "Epoch 72/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0372 - accuracy: 0.9930 - val_loss: 0.0522 - val_accuracy: 0.9870\n",
            "Epoch 73/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0478 - accuracy: 0.9890 - val_loss: 0.4691 - val_accuracy: 0.8340\n",
            "Epoch 74/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0532 - accuracy: 0.9870 - val_loss: 0.1798 - val_accuracy: 0.9410\n",
            "Epoch 75/100\n",
            "125/125 [==============================] - 7s 57ms/step - loss: 0.0410 - accuracy: 0.9900 - val_loss: 0.0435 - val_accuracy: 0.9920\n",
            "Epoch 76/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0520 - accuracy: 0.9850 - val_loss: 0.0654 - val_accuracy: 0.9840\n",
            "Epoch 77/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0262 - accuracy: 0.9970 - val_loss: 0.0270 - val_accuracy: 0.9930\n",
            "Epoch 78/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0621 - accuracy: 0.9830 - val_loss: 0.2658 - val_accuracy: 0.8950\n",
            "Epoch 79/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0504 - accuracy: 0.9860 - val_loss: 0.3625 - val_accuracy: 0.8830\n",
            "Epoch 80/100\n",
            "125/125 [==============================] - 7s 57ms/step - loss: 0.0368 - accuracy: 0.9930 - val_loss: 0.0136 - val_accuracy: 0.9980\n",
            "Epoch 81/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0451 - accuracy: 0.9870 - val_loss: 0.0665 - val_accuracy: 0.9830\n",
            "Epoch 82/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0538 - accuracy: 0.9890 - val_loss: 0.0421 - val_accuracy: 0.9870\n",
            "Epoch 83/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0388 - accuracy: 0.9910 - val_loss: 0.0134 - val_accuracy: 0.9990\n",
            "Epoch 84/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0258 - accuracy: 0.9960 - val_loss: 0.0628 - val_accuracy: 0.9830\n",
            "Epoch 85/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0186 - accuracy: 0.9980 - val_loss: 0.0148 - val_accuracy: 0.9930\n",
            "Epoch 86/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0283 - accuracy: 0.9950 - val_loss: 0.0726 - val_accuracy: 0.9780\n",
            "Epoch 87/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0287 - accuracy: 0.9960 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "125/125 [==============================] - 8s 60ms/step - loss: 0.0293 - accuracy: 0.9920 - val_loss: 0.0118 - val_accuracy: 0.9990\n",
            "Epoch 89/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0316 - accuracy: 0.9910 - val_loss: 0.0400 - val_accuracy: 0.9880\n",
            "Epoch 90/100\n",
            "125/125 [==============================] - 7s 59ms/step - loss: 0.0261 - accuracy: 0.9950 - val_loss: 0.0816 - val_accuracy: 0.9690\n",
            "Epoch 91/100\n",
            "125/125 [==============================] - 7s 60ms/step - loss: 0.0358 - accuracy: 0.9930 - val_loss: 0.0320 - val_accuracy: 0.9930\n",
            "Epoch 92/100\n",
            "125/125 [==============================] - 7s 57ms/step - loss: 0.0322 - accuracy: 0.9960 - val_loss: 0.0076 - val_accuracy: 0.9990\n",
            "Epoch 93/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0287 - accuracy: 0.9940 - val_loss: 0.0557 - val_accuracy: 0.9820\n",
            "Epoch 94/100\n",
            "125/125 [==============================] - 7s 58ms/step - loss: 0.0162 - accuracy: 0.9960 - val_loss: 0.0365 - val_accuracy: 0.9910\n",
            "Epoch 95/100\n",
            "125/125 [==============================] - 8s 67ms/step - loss: 0.0566 - accuracy: 0.9880 - val_loss: 0.0314 - val_accuracy: 0.9940\n",
            "Epoch 96/100\n",
            "125/125 [==============================] - 8s 61ms/step - loss: 0.0216 - accuracy: 0.9950 - val_loss: 0.0163 - val_accuracy: 0.9960\n",
            "Epoch 97/100\n",
            "125/125 [==============================] - 8s 62ms/step - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "125/125 [==============================] - 7s 60ms/step - loss: 0.0129 - accuracy: 0.9990 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "125/125 [==============================] - 7s 60ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 9.3482e-04 - val_accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "125/125 [==============================] - 7s 59ms/step - loss: 0.0179 - accuracy: 0.9980 - val_loss: 0.0347 - val_accuracy: 0.9930\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.0292 - accuracy: 1.0000\n",
            "final loss 0.029249878600239754, final acc 1.0\n",
            "7/7 [==============================] - 0s 19ms/step - loss: 3.1204 - accuracy: 0.2885\n",
            "test loss 3.1204185485839844, test acc 0.2884615361690521\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGls3sgs102I",
        "outputId": "5bbcd502-320b-49e2-c81d-eba9ab592dd6"
      },
      "source": [
        "pred_classes = model.predict(test).argmax(axis=-1) + 1\n",
        "\n",
        "pred_classes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([10,  3,  1,  1, 10,  3,  8,  9,  3,  2,  7,  9, 10,  9,  3,  6,  3,\n",
              "        8,  3,  7, 10,  9, 10,  3,  4,  8,  7,  3,  1,  5, 10,  1,  1,  8,\n",
              "       10,  3,  4,  7,  7,  7,  8,  3, 10,  7,  8,  8,  8,  5, 10,  1,  1,\n",
              "        1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "2oxGnyV-5GuZ",
        "outputId": "8e969a60-204b-46c9-bdee-1e14206ab25f"
      },
      "source": [
        "#read a file and convert to an array and back to an image\n",
        "img = load_img('/content/data_preprocessed/test/ii/a3ef6ed0-ce5d-11eb-b317-38f9d35ea60f.png')\n",
        "img_numpy_array = img_to_array(img)\n",
        "print('shape', img_numpy_array.shape)\n",
        "#print(img_numpy_array)\n",
        "img_pil_from_numpy_array = array_to_img(img_numpy_array)\n",
        "img_pil_from_numpy_array"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape (275, 269, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ0AAAETCAIAAACjrMS0AAAFcklEQVR4nO3d3VqbQBSGUdKn93/L9CAWKQG+4SdaZ691WHuQIC97BqI+xnEcgF2/vvsFwA+gE8h0AplOINMJZDqBTCeQ6QQynUCmE8h0AplOINMJZDqBTCeQ6QQynUCmE8h0AplOINMJZDqBTCeQ6QQynUCmE8h0AplOINMJZDqBTCeQ6QQynUCmE8h0AplOINMJZDqBTCeQ6QSy39/9AniLx+Px+o/jOH79K+mDeQKZTiDTCWQ6gUwnkPV5v2v1bs/ghg9ndThPtiJ5fmnnq7Clz3nCgkF60aO/I9gyMfp719HisBQ8Ald0uO6C2+mkhNcZa592SNFOnCUc0uE+fr7y1gO36Hye2K3ucBFp13knw3Yqdc6SOu/0ffrvBK4r0YnVFxeV6KQyi65blOjEucJFPXfy+GvnP3zl6/kPOQKNeu6EFlJpoROkkumkc+M4ut13XfVOKlxKK7zHd6veCbTQCcPgJ6ITnUCmE8h0YptLppP+td8XdsnY0nMnnhtMHIqLeu4E7qKTKjyYv0In/MMWZZVOINMJS0bKK51AppNa7ObP0QlkOoFMJ5DphBVueS103ok9K7fovBNOM1LmdAKZTthkpEx0UpFt21E6gUwnkOkEMp2wx1b+SSeQ6WQYSl412z9gX/DgvOq8E9/jfW4QN+q8E27hcqMTyHQCmU4g0wlknXfifg636LwTuIVOqjNyW/TfifMgcoii/juB63QCmU4++GjGvuLHRye0qpyKTiD7/d0v4CvM7+dUvihyWrl54h4oJ5TrBE7QCQeUXbWW2J8Mhb/B3MI8gUwnnyrPHLc39umED1LZoRPIdMInf11+S5VO/O5DrqjSCVyhE46pOXKrPGekRc0GWpgnkOkEMp1AppMla3Re6YTDCl5KCnXiSXM0/vXdL+S/U6gTOE0nkOlkRcH194Kl14JOWCeVOZ9bYYWJulCrE7/wjnOsu1hyBXmlE8h0AlnRTiwtOKRoJ1xU7UKjk3XVzgP26QQynbDkSfwrnUCmE8h0AplOINMJZDqBrNbn6mnkBxAWzJNNzg8mRTvxKI1DrLtYZ5zO1e1kGilOiHOex63IZC667oJDdAKZTiDTSZUVNlfoZI8tPk9173c9KWGL+4Fz5glkOglcTRmsu9jiAjFnnkCmE8h0AplOuKTINkYnkFXvxJ9L3+KwzFXvZGhYORRZWpxW4fjopEmFU4EdOoFMJ5D53AqbfGR4opNhHEfnwSqHZWLdBZlOINMJZDqBTCetbGor08kw+CwTiU4g0wmbfJh64jkjgafyg3kCLXTywQKDHTqBzP6EPZX3JHPmyae49HLSlKUTyHTyDyOFVTqBTCeQud/FnsVCtOyyUycEZduYs+6CTCeQ6QQynbDH5uRJJ5DpZMkjeV7pZIVUWNAJV1X4ETedsMnYnOgEMp1AphPIdAKZTtbFX4Voj1uKTvZUuONJC52cZ6TUoRPIdAKZTiDTySW2KEXoBDKdQKYTyHQCmd9z9xav+/sf92jfLYo58yT4cec376CTq4pfd4tcR3QCmU4gs4/PnkuLlvVVT2uwcRx7ejsXmSe32TmriiziO6YTyKy7bmB90j3zBDKdsM6QnLPuei87+D6YJ62c8ZWZJ++iq56YJwe0n/oi6czDd/SEgo8Ut95yr+93wTwhc+/L/uSM6SK6OIE6u7jKY6KTSzoLgy3WXZDphHUWXXM6gUwnkOmES4osz3QCmU4g87kVmuyvr7o/izxnZE+R7Udk3QWZTiDTCWQ6gcz9Llqt7umLnD/mCecViWQwT6CFeQKZTiDTCWQ6gUwnkOkEMp1AphPIdAKZTiDTCWQ6gUwnkOkEMp1AphPIdAKZTiDTCWQ6gewP+oQWm5VucvkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=269x275 at 0x7FFA6D5A5F50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "2_CzZB7t5awv",
        "outputId": "2c412048-9379-4332-83a4-6e7841981074"
      },
      "source": [
        "for i,j in test:\n",
        "  img_pil_from_numpy_array = array_to_img(i[0,:,:,:])\n",
        "  display(img_pil_from_numpy_array)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAABJElEQVR4nM2VvQ2EMAyFnRN0lCkYhC2go2UERmEMNqBkBLIHDRWCLhJXhMtxIQE7ItJ9lQWxn+3HD9u2DcIwTRPn/IVPqOvaQ4b90QQkhBCccwAIJaAJuCKFfYLpA/sFX1cIoQJzgiRJ1nXFlLgenbG9sjnBsiybjTzPjSvIUa5MJu3E4NuBtV9123ULwzAMKjA9kFJGUQSHJfrh9EBVP1MUBUlgnuddydVmqAmMQ9aYhFMgTVO/iliBcRwBoGkav7r3j6k+oeKyLEmPqU68cdLbapTJ+ijQTdZt3Qioc1JKUvVjQ/bX6kjf93Ecw2kI5Oqic6YLTzN0WlVVKmjb1qPQvUAgQv309dqxAs9/i54ilAD2RTsnUAnlQZZlNIGu60gCmjcL9FFtsfSzpQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32 at 0x7FFA6D0695D0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAA20lEQVR4nO2UMQ6DMAxFm65cIFtyjKwcijOEW2TlBCzsXITBrCxRBiR3sFohWrU2VdRI5U8m+OfBj0Eh4iWnrll3PwECQEopE0D92RTVdS0FFBPROI5UzPOsNvrsRJ601tTvvWdaSAcjUoprFB/yMAwiiwzQ9z0ViGit3d2dpunZIo7oEQ4zpfwf2rqux5zGmN1K0zQv+kQzR5lQYYzh9IsjEo81/9m99/quEAJzBy6gqirnHADEGAHAOUfrAPDeWMzPbqdlWZidpb5BqYCu67aXbdt+DzjP4PeAG7JWlcovHq19AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32 at 0x7FFA6C9EB950>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAABY0lEQVR4nO1WMY6DMBBcn9OnQyjUUSrSUJkGKVJeQJMX5DW8BBo68g4kagqgMU0+kOwVq7MiLsaGQHHSTWXjxbPjXQ8wRIQ18bXq7v8Ef59gs9mYY2bvzhizafE5Cvq+t9x9DsHxeHQcZ8L1RGtkWQYAbdvav4KIVgR5ngMAJT4VBgLKepLQAbRdxBgDgO12i5+5obbIxF9VFU3run5dPRwOlgRj3UYiKGDQlwu0aRAEiBiGIU3TNLVMeQht9X9yV0+6rlPj3W5nWWStgrIsAUApAADP89T4crlYCtAS+L4/8tormQHjAgcB+/1ejc/ns80RTSOAxa2CajDgEEIYpSsY3PR2u/0+0iiKrtfrMjVAxKIoLJN9iwkuJqVsmkYIEcexlHJJAtLKOacp51wdgBDi8Xh8RAAAz+dTt3q/3wHgdDrpAsyfzCRJyPXegvzcdV1dgNkUu66bcG9nEHyI1f/svgHd6gtDP6r63QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32 at 0x7FFA69E5FDD0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAABXUlEQVR4nO1WMY7CMBBcn6JEsqKQgioKkouUPCIVP+FJ1Kn8EB5hN9DwAKS4iUK1V/jgzMGFMTqkO+m22qxmdrzetR3BzPRKe3tp9n+BXyGQgDghhHdipw6toKoqZmbmxWJxq22t/Y6IVrDdbu/GhRDTNUECUsphGLy/Xq/x7EREDNhsNvNO13VhfLPZPORCAlprpZSUsmmaTya2OKDG827QeYT8zkD7A07RarXK8zxMt9/v0ZUhuHD5t5/T9riCvu+dc1+CaZoi2QkZ07IsicgYEwZPpxMoAE1C27ZXk4exPsAIKEmS3W73nADa5BAGDqg39LK73KbWWmbOsgwkQsUSkTHG+/4wg0QGexBeQXQ+AT8pcEUgYubj8ai1RvAR7bo0g4Mb6SH++TfZOTeOIwX9v2voi3axuq69UxSFL2K5XE4RnujBfD4PPw+HwxQ+VkApFYWPbnKs/f0/u3eXVYm+/BnNNAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32 at 0x7FFA6D0695D0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAA9klEQVR4nO2UPY6EMAyFMT/i5xhBXCFNTsoBfBF6DoFoXLmiQFEkbxHtNLuYEQOa0YhXJvb7ouQ5ICLJlUovdf96AAA4514lyLaeaW/bVnEQEQ1grRWRaZqUmnmerbXKQeGsmAL8b3XaI28d9H0pAoBn+td1PQiIWpYlTbWauq6PA8ZxDCEMw6Bb6NpMUbyioii890p/nuchhCOAB+PFHGtXJCJZlu1adF2n7J4waFVVKVk6bZK3tD9oRJQkCTMzMxHBr5xzRMTMO/36XygiTdPESmOMMQYR4zoiIqIxJu72fX/tZ+e9L8vyr9sHvMENuAE34PMBP+3mKDQluztGAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32 at 0x7FFA66517310>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAABHklEQVR4nO1Wuw2DMBD1i1IwAhuwAhswAowAC1CyAD0SE7AHY1AyAg0dTmHFIvad7UAokPIqON/v+T4AKaW4Eo9Lvf8D/AOEAwCAnwWY59nwLoTg5gkHBg0wrWyJRhADAGma6lcpZdd1hk7TNLSx/ESWZbbQkNiGpCsFglrIDRgSRxnoK9q3RFmWNPcdhmHgjo63aRzH+rkoCk6Nrv7+Bkj6thDAuq5RFBmuWAbc4DgwjiPhh2OgEwxnYKsJjoEq2rZtXLJVVXFHZq7siL8z4lIL7FR/Fzl2yTRNXvOnwy+Auq6XZeF0kiTxrjIPg7Zt3QoaeZ5/HUBl1/e949QL/7p2r3tD01Zja6Bx8s/sJt9kBT00VwUQxwbtJO4f4AUXBNnO+SDXcQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32 at 0x7FFA71CAA910>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAApElEQVR4nO2VsQ3EIAxF+aeDlh1o2IL5kJiJDRiDAegpfEWkKEpIcncQKYr8S2z5YRtjEJG4Uq9LozOAAQxgwH0BIYSRBGppe+6c8943nY+F5j6otSqlliag7Xmqdg+klKUUAJ3RhRDvPYPWOuc8MXq23snV+gFHz3SqTIxxrtUf2s1gVffBTd6GIyJrLYCU0m+Eb4ZgJWNM7xwM1CM/OwY8DfAB7Yfvoant2iYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32 at 0x7FFA6C9EB950>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}