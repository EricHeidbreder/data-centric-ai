{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/EricHeidbreder/data-centric-ai/blob/eric_h/data_centric_ai_comp_copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\r\n",
        "import requests\r\n",
        "import tarfile\r\n",
        "import io\r\n",
        "import numpy as np\r\n",
        "from PIL import Image, ImageTk\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "import pandas as pd\r\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\r\n",
        "from tensorflow.python.keras.preprocessing import dataset_utils\r\n",
        "import os\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "import json\r\n",
        "import sys\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "# from google.colab.patches import cv2_imshow\r\n",
        "\r\n",
        "import shutil\r\n",
        "import random"
      ],
      "outputs": [],
      "metadata": {
        "id": "L965aAYLKcWv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "train_datagen = ImageDataGenerator(rotation_range=20,\r\n",
        "                                   width_shift_range=0.2,\r\n",
        "                                   height_shift_range=0.2,\r\n",
        "                                   rescale=1/255,\r\n",
        "                                   shear_range=0.2,\r\n",
        "                                   zoom_range=0.2,\r\n",
        "                                   horizontal_flip=False)\r\n",
        "\r\n",
        "valid_datagen = ImageDataGenerator(rescale=1/255)"
      ],
      "outputs": [],
      "metadata": {
        "id": "yGI7F9v5LZyh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        " batch_size = len(os.listdir('.\\\\data_sorted\\\\train\\\\ii'))\r\n",
        " batch_size"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOLzY53-WT9n",
        "outputId": "9f5d4af2-2b7c-40ae-c325-64060404f9d8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "data_sorted_path = '.\\\\data_sorted_copy'\r\n",
        "# data_sorted_path = '.\\\\data_sorted'\r\n",
        "class_folders = os.listdir(data_sorted_path)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "for folder in class_folders:\r\n",
        "  # Don't do this in the test folder\r\n",
        "  if folder not in [\r\n",
        "    'test',\r\n",
        "    '.DS_Store',\r\n",
        "    # TODO: remove val after subgroups are added to that folder\r\n",
        "    'val'\r\n",
        "  ]:\r\n",
        "    class_folders = os.listdir(os.path.join(data_sorted_path, folder))\r\n",
        "  else:\r\n",
        "    continue\r\n",
        "\r\n",
        "  # Iterates through the subfolders in each class. \r\n",
        "  # Makes sure there are an equal number of items from each subgroup within the classes by randomly sampling the smaller subgroups\r\n",
        "  for class_folder in class_folders:\r\n",
        "    \r\n",
        "    # Gather initial information about the class path and the number of subgroups\r\n",
        "    if class_folder not in ['.DS_Store', 'junk_vals']:\r\n",
        "    # if class_folder == 'i':\r\n",
        "      class_path = os.path.join(data_sorted_path, folder, class_folder)\r\n",
        "      class_subgroups = os.listdir(class_path)\r\n",
        "      max_subgroup_len = 0\r\n",
        "\r\n",
        "      # Need to get the max number of files in a subgroup folder, so we can get them all to match later\r\n",
        "      for class_folder_subgroup in class_subgroups:\r\n",
        "        max_subgroup_len = max(len(os.listdir(os.path.join(data_sorted_path, folder, class_folder, class_folder_subgroup))), max_subgroup_len)\r\n",
        "\r\n",
        "      # Get the subgroup path (example i_lowercase, i_ruled, etc.)\r\n",
        "      for class_folder_subgroup in class_subgroups:\r\n",
        "        class_subgroup_path = os.path.join(data_sorted_path, folder, class_folder, class_folder_subgroup)\r\n",
        "\r\n",
        "        # If the subgroup isn't the max length, determine how many copies we need to make to get it to match\r\n",
        "        if len(os.listdir(class_subgroup_path)) < max_subgroup_len:\r\n",
        "          subgroup_images = os.listdir(class_subgroup_path)\r\n",
        "          num_subgroup_images = len(subgroup_images)\r\n",
        "          num_copies_to_make = max_subgroup_len - num_subgroup_images\r\n",
        "\r\n",
        "          # Make the copies using random sampling of the existing images\r\n",
        "          for i in range(num_copies_to_make):\r\n",
        "            random_image = subgroup_images[random.randint(0, num_subgroup_images - 1)]\r\n",
        "            shutil.copyfile(os.path.join(class_subgroup_path, random_image), class_subgroup_path+f'/copy_{i}_{random_image}')\r\n",
        "\r\n",
        "    else:\r\n",
        "        continue"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class_names=[\"i\", \"ii\", \"iii\", \"iv\", \"v\", \"vi\", \"vii\", \"viii\", \"ix\", \"x\"]\r\n",
        "\r\n",
        "for class_name in class_names:\r\n",
        "  train_generator = train_datagen.flow_from_directory(\r\n",
        "    './data_sorted/train',\r\n",
        "    target_size=(32,32),\r\n",
        "    batch_size=500,\r\n",
        "    classes=[class_name],\r\n",
        "    save_to_dir='./data_preprocessed/train/'+class_name,\r\n",
        "    save_prefix='aug',\r\n",
        "    shuffle=True\r\n",
        "  )\r\n",
        "  batch = next(train_generator)\r\n",
        " \r\n",
        "for class_name in class_names:\r\n",
        "  batch_size = len([f for f in os.listdir('./data_sorted/val/'+ class_name) if os.path.isfile(os.path.join('./data_sorted/val/' + class_name, f))])\r\n",
        "  validation_generator = valid_datagen.flow_from_directory(\r\n",
        "        './data_sorted/val',\r\n",
        "        target_size=(32, 32),\r\n",
        "        class_mode='categorical',\r\n",
        "        classes=[class_name],\r\n",
        "        batch_size=batch_size,\r\n",
        "        shuffle=False,\r\n",
        "        save_to_dir='./data_preprocessed/val/'+class_name)  \r\n",
        "  batch = next(validation_generator)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWOantahLe7c",
        "outputId": "7b536468-8196-4358-f23d-5a9cee06f134"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "directory = \"./data_preprocessed\"\r\n",
        "user_data = directory + \"/train\"\r\n",
        "valid_data = directory + \"/val\"\r\n",
        "test_data = directory + \"/test\" # this can be the label book, or any other test set you create\r\n",
        "\r\n",
        "### DO NOT MODIFY BELOW THIS LINE, THIS IS THE FIXED MODEL ###\r\n",
        "batch_size = 8\r\n",
        "tf.random.set_seed(123)\r\n",
        "\r\n",
        "\r\n",
        "if __name__ == \"__main__\":\r\n",
        "    train = tf.keras.preprocessing.image_dataset_from_directory(\r\n",
        "        user_data,# + '/train',\r\n",
        "        labels=\"inferred\",\r\n",
        "        label_mode=\"categorical\",\r\n",
        "        class_names=[\"i\", \"ii\", \"iii\", \"iv\", \"v\", \"vi\", \"vii\", \"viii\", \"ix\", \"x\"],\r\n",
        "        shuffle=True,\r\n",
        "        seed=123,\r\n",
        "        batch_size=batch_size,\r\n",
        "        image_size=(32, 32),\r\n",
        "    )\r\n",
        "\r\n",
        "    valid = tf.keras.preprocessing.image_dataset_from_directory(\r\n",
        "        valid_data,# + '/val',\r\n",
        "        labels=\"inferred\",\r\n",
        "        label_mode=\"categorical\",\r\n",
        "        class_names=[\"i\", \"ii\", \"iii\", \"iv\", \"v\", \"vi\", \"vii\", \"viii\", \"ix\", \"x\"],\r\n",
        "        shuffle=True,\r\n",
        "        seed=123,\r\n",
        "        batch_size=batch_size,\r\n",
        "        image_size=(32, 32),\r\n",
        "    )\r\n",
        "\r\n",
        "    total_length = ((train.cardinality() + valid.cardinality()) * batch_size).numpy()\r\n",
        "    if total_length > 10_000:\r\n",
        "        print(f\"Dataset size larger than 10,000. Got {total_length} examples\")\r\n",
        "        sys.exit()\r\n",
        "\r\n",
        "    test = tf.keras.preprocessing.image_dataset_from_directory(\r\n",
        "        test_data,\r\n",
        "        labels=\"inferred\",\r\n",
        "        label_mode=\"categorical\",\r\n",
        "        class_names=[\"i\", \"ii\", \"iii\", \"iv\", \"v\", \"vi\", \"vii\", \"viii\", \"ix\", \"x\"],\r\n",
        "        shuffle=False,\r\n",
        "        seed=123,\r\n",
        "        batch_size=batch_size,\r\n",
        "        image_size=(32, 32),\r\n",
        "    )\r\n",
        "\r\n",
        "    base_model = tf.keras.applications.ResNet50(\r\n",
        "        input_shape=(32, 32, 3),\r\n",
        "        include_top=False,\r\n",
        "        weights=None,\r\n",
        "    )\r\n",
        "    base_model = tf.keras.Model(\r\n",
        "        base_model.inputs, outputs=[base_model.get_layer(\"conv2_block3_out\").output]\r\n",
        "    )\r\n",
        "\r\n",
        "    inputs = tf.keras.Input(shape=(32, 32, 3))\r\n",
        "    x = tf.keras.applications.resnet.preprocess_input(inputs)\r\n",
        "    x = base_model(x)\r\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\r\n",
        "    x = tf.keras.layers.Dense(10)(x)\r\n",
        "    model = tf.keras.Model(inputs, x)\r\n",
        "\r\n",
        "    model.compile(\r\n",
        "        optimizer=tf.keras.optimizers.Adam(lr=0.0001),\r\n",
        "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\r\n",
        "        metrics=[\"accuracy\"],\r\n",
        "    )\r\n",
        "    model.summary()\r\n",
        "    loss_0, acc_0 = model.evaluate(valid)\r\n",
        "    print(f\"loss {loss_0}, acc {acc_0}\")\r\n",
        "\r\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\r\n",
        "        \"best_model\",\r\n",
        "        monitor=\"val_accuracy\",\r\n",
        "        mode=\"max\",\r\n",
        "        save_best_only=True,\r\n",
        "        save_weights_only=True,\r\n",
        "    )\r\n",
        "\r\n",
        "    history = model.fit(\r\n",
        "        train,\r\n",
        "        validation_data=valid,\r\n",
        "        epochs=100,\r\n",
        "        callbacks=[checkpoint],\r\n",
        "    )\r\n",
        "\r\n",
        "    model.load_weights(\"best_model\")\r\n",
        "\r\n",
        "    loss, acc = model.evaluate(valid)\r\n",
        "    print(f\"final loss {loss}, final acc {acc}\")\r\n",
        "\r\n",
        "    test_loss, test_acc = model.evaluate(test)\r\n",
        "    print(f\"test loss {test_loss}, test acc {test_acc}\")\r\n",
        "\r\n",
        "   "
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxTkqwrnL1Dp",
        "outputId": "983cba2c-1c97-49bc-efc2-7155c23b6b98"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#view predictions\r\n",
        "pred_classes = model.predict(test).argmax(axis=-1) + 1\r\n",
        "\r\n",
        "pred_classes"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qr1LuAw0P3AT",
        "outputId": "a4bfea92-f58f-4084-d7d8-6e532c949738"
      }
    }
  ]
}