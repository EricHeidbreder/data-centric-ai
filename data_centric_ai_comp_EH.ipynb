{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/EricHeidbreder/data-centric-ai/blob/eric_h/data_centric_ai_comp_copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "\r\n",
        "import requests\r\n",
        "import tarfile\r\n",
        "import io\r\n",
        "import numpy as np\r\n",
        "from PIL import Image, ImageTk\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "import pandas as pd\r\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\r\n",
        "from tensorflow.python.keras.preprocessing import dataset_utils\r\n",
        "import os\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "import json\r\n",
        "import sys\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "# from google.colab.patches import cv2_imshow\r\n",
        "\r\n",
        "import shutil\r\n",
        "import random"
      ],
      "outputs": [],
      "metadata": {
        "id": "L965aAYLKcWv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "source": [
        "# Give you a fresh start on preprocessed data\r\n",
        "shutil.rmtree('./data_preprocessed', ignore_errors=True)\r\n",
        "\r\n",
        "# Building the preprocessing folder structure\r\n",
        "os.mkdir('./data_preprocessed')\r\n",
        "os.mkdir('./data_preprocessed/train')\r\n",
        "os.mkdir('./data_preprocessed/val')\r\n",
        "for num in ['i', 'ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'x']:\r\n",
        "    os.mkdir(f'./data_preprocessed/train/{num}')\r\n",
        "    os.mkdir(f'./data_preprocessed/val/{num}')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "source": [
        "# get the test data into its own folder\r\n",
        "\r\n",
        "test_path = './data_preprocessed/test'\r\n",
        "\r\n",
        "tar_file = tarfile.open('./label_book.tar.gz')\r\n",
        "tar_file.extractall(test_path)\r\n",
        "tar_file.close()\r\n",
        "\r\n",
        "orig_base_path = os.path.join(test_path, 'label_book')\r\n",
        "for folder in os.listdir(orig_base_path):\r\n",
        "    if folder != '.DS_Store':\r\n",
        "        orig_path = os.path.join(orig_base_path, folder)\r\n",
        "        new_path = test_path\r\n",
        "        shutil.move(orig_path, new_path)\r\n",
        "\r\n",
        "shutil.rmtree(orig_base_path)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "source": [
        "train_datagen = ImageDataGenerator(rotation_range=20,\r\n",
        "                                   width_shift_range=0.2,\r\n",
        "                                   height_shift_range=0.2,\r\n",
        "                                   rescale=1/255,\r\n",
        "                                   shear_range=0.2,\r\n",
        "                                   zoom_range=0.2,\r\n",
        "                                   horizontal_flip=False)\r\n",
        "\r\n",
        "valid_datagen = ImageDataGenerator(rescale=1/255)"
      ],
      "outputs": [],
      "metadata": {
        "id": "yGI7F9v5LZyh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "batch_size = len(os.listdir('.\\\\data_sorted\\\\train\\\\ii'))\r\n",
        "batch_size"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOLzY53-WT9n",
        "outputId": "9f5d4af2-2b7c-40ae-c325-64060404f9d8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "data_sorted_path = '.\\\\data_sorted_copy'\r\n",
        "class_folders = os.listdir(data_sorted_path)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "for folder in class_folders:\r\n",
        "  # Don't do this in the test folder\r\n",
        "  if folder not in [\r\n",
        "    'test',\r\n",
        "    '.DS_Store',\r\n",
        "    # TODO: remove val after subgroups are added to that folder\r\n",
        "    'val'\r\n",
        "  ]:\r\n",
        "    class_folders = os.listdir(os.path.join(data_sorted_path, folder))\r\n",
        "  else:\r\n",
        "    continue\r\n",
        "\r\n",
        "  # Iterates through the subfolders in each class. \r\n",
        "  # Makes sure there are an equal number of items from each subgroup within the classes by randomly sampling the smaller subgroups\r\n",
        "  for class_folder in class_folders:\r\n",
        "    \r\n",
        "    # Gather initial information about the class path and the number of subgroups\r\n",
        "    if class_folder not in ['.DS_Store', 'junk_vals']:\r\n",
        "    # if class_folder == 'i':\r\n",
        "      class_path = os.path.join(data_sorted_path, folder, class_folder)\r\n",
        "      class_subgroups = os.listdir(class_path)\r\n",
        "      max_subgroup_len = 0\r\n",
        "\r\n",
        "      # Need to get the max number of files in a subgroup folder, so we can get them all to match later\r\n",
        "      for class_folder_subgroup in class_subgroups:\r\n",
        "        max_subgroup_len = max(len(os.listdir(os.path.join(data_sorted_path, folder, class_folder, class_folder_subgroup))), max_subgroup_len)\r\n",
        "\r\n",
        "      # Get the subgroup path (example i_lowercase, i_ruled, etc.)\r\n",
        "      for class_folder_subgroup in class_subgroups:\r\n",
        "        class_subgroup_path = os.path.join(data_sorted_path, folder, class_folder, class_folder_subgroup)\r\n",
        "\r\n",
        "        # If the subgroup isn't the max length, determine how many copies we need to make to get it to match\r\n",
        "        if len(os.listdir(class_subgroup_path)) < max_subgroup_len:\r\n",
        "          subgroup_images = os.listdir(class_subgroup_path)\r\n",
        "          num_subgroup_images = len(subgroup_images)\r\n",
        "          num_copies_to_make = max_subgroup_len - num_subgroup_images\r\n",
        "\r\n",
        "          # Make the copies using random sampling of the existing images\r\n",
        "          for i in range(num_copies_to_make):\r\n",
        "            random_image = subgroup_images[random.randint(0, num_subgroup_images - 1)]\r\n",
        "            shutil.copyfile(os.path.join(class_subgroup_path, random_image), class_subgroup_path+f'/copy_{i}_{random_image}')\r\n",
        "\r\n",
        "    else:\r\n",
        "        continue"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "source": [
        "class_names=[\"i\", \"ii\", \"iii\", \"iv\", \"v\", \"vi\", \"vii\", \"viii\", \"ix\", \"x\"]\r\n",
        "\r\n",
        "for class_name in class_names:\r\n",
        "  train_generator = train_datagen.flow_from_directory(\r\n",
        "    './data_sorted_copy/train',\r\n",
        "    target_size=(32,32),\r\n",
        "    batch_size=500,\r\n",
        "    classes=[class_name],\r\n",
        "    save_to_dir='./data_preprocessed/train/'+class_name,\r\n",
        "    save_prefix='aug',\r\n",
        "    shuffle=True\r\n",
        "  )\r\n",
        "  batch = next(train_generator)\r\n",
        " \r\n",
        "for class_name in class_names:\r\n",
        "  batch_size = len([f for f in os.listdir('./data_sorted_copy/val/'+ class_name) if os.path.isfile(os.path.join('./data_sorted_copy/val/' + class_name, f))])\r\n",
        "  validation_generator = valid_datagen.flow_from_directory(\r\n",
        "        './data_sorted_copy/val',\r\n",
        "        target_size=(32, 32),\r\n",
        "        class_mode='categorical',\r\n",
        "        classes=[class_name],\r\n",
        "        batch_size=batch_size,\r\n",
        "        shuffle=False,\r\n",
        "        save_to_dir='./data_preprocessed/val/'+class_name)  \r\n",
        "  batch = next(validation_generator)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 513 images belonging to 1 classes.\n",
            "Found 270 images belonging to 1 classes.\n",
            "Found 258 images belonging to 1 classes.\n",
            "Found 549 images belonging to 1 classes.\n",
            "Found 339 images belonging to 1 classes.\n",
            "Found 210 images belonging to 1 classes.\n",
            "Found 256 images belonging to 1 classes.\n",
            "Found 375 images belonging to 1 classes.\n",
            "Found 312 images belonging to 1 classes.\n",
            "Found 345 images belonging to 1 classes.\n",
            "Found 82 images belonging to 1 classes.\n",
            "Found 82 images belonging to 1 classes.\n",
            "Found 79 images belonging to 1 classes.\n",
            "Found 84 images belonging to 1 classes.\n",
            "Found 82 images belonging to 1 classes.\n",
            "Found 82 images belonging to 1 classes.\n",
            "Found 77 images belonging to 1 classes.\n",
            "Found 83 images belonging to 1 classes.\n",
            "Found 81 images belonging to 1 classes.\n",
            "Found 81 images belonging to 1 classes.\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWOantahLe7c",
        "outputId": "7b536468-8196-4358-f23d-5a9cee06f134"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "source": [
        "directory = \"./data_preprocessed\"\r\n",
        "user_data = directory + \"/train\"\r\n",
        "valid_data = directory + \"/val\"\r\n",
        "test_data = directory + \"/test\" # this can be the label book, or any other test set you create\r\n",
        "\r\n",
        "### DO NOT MODIFY BELOW THIS LINE, THIS IS THE FIXED MODEL ###\r\n",
        "batch_size = 8\r\n",
        "tf.random.set_seed(123)\r\n",
        "\r\n",
        "\r\n",
        "if __name__ == \"__main__\":\r\n",
        "    train = tf.keras.preprocessing.image_dataset_from_directory(\r\n",
        "        user_data,# + '/train',\r\n",
        "        labels=\"inferred\",\r\n",
        "        label_mode=\"categorical\",\r\n",
        "        class_names=[\"i\", \"ii\", \"iii\", \"iv\", \"v\", \"vi\", \"vii\", \"viii\", \"ix\", \"x\"],\r\n",
        "        shuffle=True,\r\n",
        "        seed=123,\r\n",
        "        batch_size=batch_size,\r\n",
        "        image_size=(32, 32),\r\n",
        "    )\r\n",
        "\r\n",
        "    valid = tf.keras.preprocessing.image_dataset_from_directory(\r\n",
        "        valid_data,# + '/val',\r\n",
        "        labels=\"inferred\",\r\n",
        "        label_mode=\"categorical\",\r\n",
        "        class_names=[\"i\", \"ii\", \"iii\", \"iv\", \"v\", \"vi\", \"vii\", \"viii\", \"ix\", \"x\"],\r\n",
        "        shuffle=True,\r\n",
        "        seed=123,\r\n",
        "        batch_size=batch_size,\r\n",
        "        image_size=(32, 32),\r\n",
        "    )\r\n",
        "\r\n",
        "    total_length = ((train.cardinality() + valid.cardinality()) * batch_size).numpy()\r\n",
        "    if total_length > 10_000:\r\n",
        "        print(f\"Dataset size larger than 10,000. Got {total_length} examples\")\r\n",
        "        sys.exit()\r\n",
        "\r\n",
        "    test = tf.keras.preprocessing.image_dataset_from_directory(\r\n",
        "        test_data,\r\n",
        "        labels=\"inferred\",\r\n",
        "        label_mode=\"categorical\",\r\n",
        "        class_names=[\"i\", \"ii\", \"iii\", \"iv\", \"v\", \"vi\", \"vii\", \"viii\", \"ix\", \"x\"],\r\n",
        "        shuffle=False,\r\n",
        "        seed=123,\r\n",
        "        batch_size=batch_size,\r\n",
        "        image_size=(32, 32),\r\n",
        "    )\r\n",
        "\r\n",
        "    base_model = tf.keras.applications.ResNet50(\r\n",
        "        input_shape=(32, 32, 3),\r\n",
        "        include_top=False,\r\n",
        "        weights=None,\r\n",
        "    )\r\n",
        "    base_model = tf.keras.Model(\r\n",
        "        base_model.inputs, outputs=[base_model.get_layer(\"conv2_block3_out\").output]\r\n",
        "    )\r\n",
        "\r\n",
        "    inputs = tf.keras.Input(shape=(32, 32, 3))\r\n",
        "    x = tf.keras.applications.resnet.preprocess_input(inputs)\r\n",
        "    x = base_model(x)\r\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\r\n",
        "    x = tf.keras.layers.Dense(10)(x)\r\n",
        "    model = tf.keras.Model(inputs, x)\r\n",
        "\r\n",
        "    model.compile(\r\n",
        "        optimizer=tf.keras.optimizers.Adam(lr=0.0001),\r\n",
        "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\r\n",
        "        metrics=[\"accuracy\"],\r\n",
        "    )\r\n",
        "    model.summary()\r\n",
        "    loss_0, acc_0 = model.evaluate(valid)\r\n",
        "    print(f\"loss {loss_0}, acc {acc_0}\")\r\n",
        "\r\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\r\n",
        "        \"best_model\",\r\n",
        "        monitor=\"val_accuracy\",\r\n",
        "        mode=\"max\",\r\n",
        "        save_best_only=True,\r\n",
        "        save_weights_only=True,\r\n",
        "    )\r\n",
        "\r\n",
        "    history = model.fit(\r\n",
        "        train,\r\n",
        "        validation_data=valid,\r\n",
        "        epochs=100,\r\n",
        "        callbacks=[checkpoint],\r\n",
        "    )\r\n",
        "\r\n",
        "    model.load_weights(\"best_model\")\r\n",
        "\r\n",
        "    loss, acc = model.evaluate(valid)\r\n",
        "    print(f\"final loss {loss}, final acc {acc}\")\r\n",
        "\r\n",
        "    test_loss, test_acc = model.evaluate(test)\r\n",
        "    print(f\"test loss {test_loss}, test acc {test_acc}\")\r\n",
        "\r\n",
        "   "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3365 files belonging to 10 classes.\n",
            "Found 813 files belonging to 10 classes.\n",
            "Found 52 files belonging to 10 classes.\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "tf.__operators__.getitem (Sl (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add (TFOpLambda)  (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "model (Functional)           (None, 8, 8, 256)         229760    \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 232,330\n",
            "Trainable params: 229,386\n",
            "Non-trainable params: 2,944\n",
            "_________________________________________________________________\n",
            "102/102 [==============================] - 18s 26ms/step - loss: 33.5076 - accuracy: 0.1121\n",
            "loss 33.73986053466797, acc 0.09963099658489227\n",
            "Epoch 1/100\n",
            "421/421 [==============================] - 17s 34ms/step - loss: 1.9718 - accuracy: 0.3165 - val_loss: 3.0699 - val_accuracy: 0.1562\n",
            "Epoch 2/100\n",
            "421/421 [==============================] - 5s 11ms/step - loss: 1.4850 - accuracy: 0.4927 - val_loss: 1.8781 - val_accuracy: 0.3678\n",
            "Epoch 3/100\n",
            "421/421 [==============================] - 5s 11ms/step - loss: 1.2535 - accuracy: 0.5789 - val_loss: 1.5345 - val_accuracy: 0.4674\n",
            "Epoch 4/100\n",
            "421/421 [==============================] - 5s 12ms/step - loss: 1.0996 - accuracy: 0.6303 - val_loss: 1.3525 - val_accuracy: 0.4859\n",
            "Epoch 5/100\n",
            "421/421 [==============================] - 5s 11ms/step - loss: 0.9632 - accuracy: 0.6868 - val_loss: 1.2201 - val_accuracy: 0.5560\n",
            "Epoch 6/100\n",
            "343/421 [=======================>......] - ETA: 0s - loss: 0.8669 - accuracy: 0.7150"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-34-8ea3c7bdb66b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     81\u001b[0m     )\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m     history = model.fit(\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mG:\\DS\\Anaconda\\envs\\ds\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mG:\\DS\\Anaconda\\envs\\ds\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mG:\\DS\\Anaconda\\envs\\ds\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mG:\\DS\\Anaconda\\envs\\ds\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mG:\\DS\\Anaconda\\envs\\ds\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32mG:\\DS\\Anaconda\\envs\\ds\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mG:\\DS\\Anaconda\\envs\\ds\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxTkqwrnL1Dp",
        "outputId": "983cba2c-1c97-49bc-efc2-7155c23b6b98"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#view predictions\r\n",
        "pred_classes = model.predict(test).argmax(axis=-1) + 1\r\n",
        "\r\n",
        "pred_classes"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qr1LuAw0P3AT",
        "outputId": "a4bfea92-f58f-4084-d7d8-6e532c949738"
      }
    }
  ]
}